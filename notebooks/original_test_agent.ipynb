{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d616501",
   "metadata": {},
   "source": [
    "# LangGraph MCP Tool-Calling Agent - Interactive Testing\n",
    "\n",
    "This notebook demonstrates how to test and deploy a LangGraph agent that connects to MCP servers hosted on Databricks.\n",
    "\n",
    "## Features\n",
    "- Author a LangGraph agent wrapped with `ResponsesAgent` for Mosaic AI compatibility\n",
    "- Call MCP tools (managed or custom)\n",
    "- Manually test the agent\n",
    "- Evaluate with Mosaic AI Agent Evaluation\n",
    "- Log and deploy the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf399125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: mlflow-skinny[databricks]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# Install required packages\n",
    "%pip install -U -qqqq langgraph uv databricks-agents mlflow-skinny[databricks] databricks-mcp databricks-langchain azure-identity azure-storage-blob azure-core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0348d5d8",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "## Step 1: Define the Agent Code\n",
    "\n",
    "Write the agent code to a local Python file using the `%%writefile` magic command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f6b49",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "## Step 0: Local Authentication Setup (for running outside Databricks)\n",
    "\n",
    "When running this notebook locally, you need to authenticate with Databricks. You have two options:\n",
    "\n",
    "### Option 1: Databricks CLI OAuth Login (Recommended for local development)\n",
    "```bash\n",
    "# Install Databricks CLI if not already installed\n",
    "pip install databricks-cli\n",
    "\n",
    "# Login using OAuth (opens browser for authentication)\n",
    "databricks auth login --host https://your-workspace.cloud.databricks.com\n",
    "```\n",
    "\n",
    "This creates a configuration profile that the SDK will automatically use.\n",
    "\n",
    "### Option 2: Environment Variables with OAuth M2M\n",
    "For automated/CI environments, use OAuth machine-to-machine with a service principal:\n",
    "```bash\n",
    "export DATABRICKS_HOST=\"https://your-workspace.cloud.databricks.com\"\n",
    "export DATABRICKS_CLIENT_ID=\"your-service-principal-client-id\"\n",
    "export DATABRICKS_CLIENT_SECRET=\"your-service-principal-secret\"\n",
    "```\n",
    "\n",
    "The code below will automatically detect and use whichever authentication method is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37d5f51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully authenticated to: https://adb-3253299566947192.12.azuredatabricks.net\n",
      "✓ Using profile: development\n",
      "✓ Auth type: metadata-service\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# Test Databricks authentication\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import os\n",
    "\n",
    "# Use the profile name (for local development)\n",
    "profile_name = os.getenv(\"DATABRICKS_CONFIG_PROFILE\", \"development\")\n",
    "\n",
    "try:\n",
    "    ws = WorkspaceClient(profile=profile_name)\n",
    "    print(f\"✓ Successfully authenticated to: {ws.config.host}\")\n",
    "    print(f\"✓ Using profile: {profile_name}\")\n",
    "    print(f\"✓ Auth type: {ws.config.auth_type}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Authentication failed: {e}\")\n",
    "    print(\"\\nTo fix, run: databricks auth login --host https://your-workspace.cloud.databricks.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba8d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile agent.py\n",
    "import asyncio\n",
    "from typing import Annotated, Any, Generator, List, Optional, Sequence, TypedDict, Union\n",
    "\n",
    "import mlflow\n",
    "import nest_asyncio\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    UCFunctionToolkit,\n",
    "    VectorSearchRetrieverTool,\n",
    ")\n",
    "from databricks_mcp import DatabricksMCPClient, DatabricksOAuthClientProvider\n",
    "from langchain.messages import AIMessage, AIMessageChunk, AnyMessage\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client as connect\n",
    "from mlflow.pyfunc import ResponsesAgent\n",
    "from mlflow.types.responses import (\n",
    "    ResponsesAgentRequest,\n",
    "    ResponsesAgentResponse,\n",
    "    ResponsesAgentStreamEvent,\n",
    "    output_to_responses_items_stream,\n",
    "    to_chat_completions_input,\n",
    ")\n",
    "from pydantic import create_model\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "############################################\n",
    "## Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "# TODO: Replace with your model serving endpoint\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# TODO: Update with your system prompt\n",
    "system_prompt = \"You are a helpful assistant that can run Python code.\"\n",
    "\n",
    "###############################################################################\n",
    "## Configure MCP Servers for your agent\n",
    "##\n",
    "## This section sets up server connections so your agent can retrieve data or take actions.\n",
    "\n",
    "## There are three connection types:\n",
    "## 1. Managed MCP servers — fully managed by Databricks (no setup required)\n",
    "## 2. External MCP servers — hosted outside Databricks but proxied through a\n",
    "##    Managed MCP server proxy (some setup required)\n",
    "## 3. Custom MCP servers — MCP servers hosted as Databricks Apps (OAuth setup required)\n",
    "##\n",
    "## Note: External MCP servers get added to the \"managed\" URL list\n",
    "## because their proxy endpoints are managed by Databricks.\n",
    "###############################################################################\n",
    "\n",
    "# TODO: Choose your MCP server connection type and fill in the appropriate URLs.\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Managed MCP Server — simplest setup\n",
    "# ---------------------------------------------------------------------------\n",
    "# Databricks manages this connection automatically using your workspace settings.\n",
    "# When running locally, it will use authentication from:\n",
    "#   1. Databricks CLI OAuth profiles (recommended - run: databricks auth login)\n",
    "#   2. Environment variables (DATABRICKS_HOST, DATABRICKS_CLIENT_ID, DATABRICKS_CLIENT_SECRET)\n",
    "#   3. Default profile from ~/.databrickscfg\n",
    "\n",
    "import os\n",
    "\n",
    "# For local development, specify the profile name if you have multiple profiles\n",
    "# Comment this out when running in Databricks notebooks\n",
    "profile_name = os.getenv(\"DATABRICKS_CONFIG_PROFILE\", \"development\")  # Use your profile name\n",
    "\n",
    "try:\n",
    "    # Try to use the specified profile (for local development)\n",
    "    workspace_client = WorkspaceClient(profile=profile_name)\n",
    "except Exception:\n",
    "    # Fall back to default authentication (works in Databricks notebooks)\n",
    "    workspace_client = WorkspaceClient()\n",
    "\n",
    "host = workspace_client.config.host\n",
    "\n",
    "# Managed MCP Servers URLS (includes both fully managed and proxied external MCP)\n",
    "# - If you're using an external MCP server, create a UC connection and flag it\n",
    "#   as an MCP connection. This reveals a proxy endpoint.\n",
    "# - Add that proxy endpoint URL to this list.\n",
    "\n",
    "MANAGED_MCP_SERVER_URLS = [\n",
    "    f\"{host}/api/2.0/mcp/functions/system/ai\",  # Default managed MCP endpoint\n",
    "    # Example for external MCP:\n",
    "    # \"https://<workspace-hostname>/api/2.0/mcp/external/{connection_name}\"\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Custom MCP Server — hosted as a Databricks App\n",
    "# ---------------------------------------------------------------------------\n",
    "# Use this if you're running your own MCP server in Databricks.\n",
    "# These require OAuth with a service principal for machine-to-machine (M2M) auth.\n",
    "#\n",
    "# Uncomment and fill in the settings below to use a custom MCP server.\n",
    "#\n",
    "# import os\n",
    "# workspace_client = WorkspaceClient(\n",
    "#     host=\"<DATABRICKS_WORKSPACE_URL>\",\n",
    "#     client_id=os.getenv(\"DATABRICKS_CLIENT_ID\"),\n",
    "#     client_secret=os.getenv(\"DATABRICKS_CLIENT_SECRET\"),\n",
    "#     auth_type=\"oauth-m2m\",  # Enables service principal authentication\n",
    "# )\n",
    "\n",
    "# Custom MCP Servers — add URLs below (not managed or proxied by Databricks)\n",
    "CUSTOM_MCP_SERVER_URLS = [\n",
    "    # Example: \"https://<custom-mcp-app-url>/mcp\"\n",
    "]\n",
    "\n",
    "\n",
    "#####################\n",
    "## MCP Tool Creation\n",
    "#####################\n",
    "\n",
    "# Define a custom LangChain tool that wraps functionality for calling MCP servers\n",
    "class MCPTool(BaseTool):\n",
    "    \"\"\"Custom LangChain tool that wraps MCP server functionality\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        description: str,\n",
    "        args_schema: type,\n",
    "        server_url: str,\n",
    "        ws: WorkspaceClient,\n",
    "        is_custom: bool = False,\n",
    "    ):\n",
    "        # Initialize the tool\n",
    "        super().__init__(name=name, description=description, args_schema=args_schema)\n",
    "        # Store custom attributes: MCP server URL, Databricks workspace client, and whether the tool is for a custom server\n",
    "        object.__setattr__(self, \"server_url\", server_url)\n",
    "        object.__setattr__(self, \"workspace_client\", ws)\n",
    "        object.__setattr__(self, \"is_custom\", is_custom)\n",
    "\n",
    "    def _run(self, **kwargs) -> str:\n",
    "        \"\"\"Execute the MCP tool\"\"\"\n",
    "        if self.is_custom:\n",
    "            # Use the async method for custom MCP servers (OAuth required)\n",
    "            return asyncio.run(self._run_custom_async(**kwargs))\n",
    "        else:\n",
    "            # Use managed MCP server via synchronous call\n",
    "            mcp_client = DatabricksMCPClient(\n",
    "                server_url=self.server_url, workspace_client=self.workspace_client\n",
    "            )\n",
    "            response = mcp_client.call_tool(self.name, kwargs)\n",
    "            return \"\".join([c.text for c in response.content])\n",
    "\n",
    "    async def _run_custom_async(self, **kwargs) -> str:\n",
    "        \"\"\"Execute custom MCP tool asynchronously\"\"\"\n",
    "        async with connect(\n",
    "            self.server_url, auth=DatabricksOAuthClientProvider(self.workspace_client)\n",
    "        ) as (\n",
    "            read_stream,\n",
    "            write_stream,\n",
    "            _,\n",
    "        ):\n",
    "            # Create an async session with the server and call the tool\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                await session.initialize()\n",
    "                response = await session.call_tool(self.name, kwargs)\n",
    "                return \"\".join([c.text for c in response.content])\n",
    "\n",
    "\n",
    "# Retrieve tool definitions from a custom MCP server (OAuth required)\n",
    "async def get_custom_mcp_tools(ws: WorkspaceClient, server_url: str):\n",
    "    \"\"\"Get tools from a custom MCP server using OAuth\"\"\"\n",
    "    async with connect(server_url, auth=DatabricksOAuthClientProvider(ws)) as (\n",
    "        read_stream,\n",
    "        write_stream,\n",
    "        _,\n",
    "    ):\n",
    "        async with ClientSession(read_stream, write_stream) as session:\n",
    "            await session.initialize()\n",
    "            tools_response = await session.list_tools()\n",
    "            return tools_response.tools\n",
    "\n",
    "\n",
    "# Retrieve tool definitions from a managed MCP server\n",
    "def get_managed_mcp_tools(ws: WorkspaceClient, server_url: str):\n",
    "    \"\"\"Get tools from a managed MCP server\"\"\"\n",
    "    mcp_client = DatabricksMCPClient(server_url=server_url, workspace_client=ws)\n",
    "    return mcp_client.list_tools()\n",
    "\n",
    "\n",
    "# Convert an MCP tool definition into a LangChain-compatible tool\n",
    "def create_langchain_tool_from_mcp(\n",
    "    mcp_tool, server_url: str, ws: WorkspaceClient, is_custom: bool = False\n",
    "):\n",
    "    \"\"\"Create a LangChain tool from an MCP tool definition\"\"\"\n",
    "    schema = mcp_tool.inputSchema.copy()\n",
    "    properties = schema.get(\"properties\", {})\n",
    "    required = schema.get(\"required\", [])\n",
    "\n",
    "    # Map JSON schema types to Python types for input validation\n",
    "    TYPE_MAPPING = {\"integer\": int, \"number\": float, \"boolean\": bool}\n",
    "    field_definitions = {}\n",
    "    for field_name, field_info in properties.items():\n",
    "        field_type_str = field_info.get(\"type\", \"string\")\n",
    "        field_type = TYPE_MAPPING.get(field_type_str, str)\n",
    "\n",
    "        if field_name in required:\n",
    "            field_definitions[field_name] = (field_type, ...)\n",
    "        else:\n",
    "            field_definitions[field_name] = (field_type, None)\n",
    "\n",
    "    # Dynamically create a Pydantic schema for the tool's input arguments\n",
    "    args_schema = create_model(f\"{mcp_tool.name}Args\", **field_definitions)\n",
    "\n",
    "    # Return a configured MCPTool instance\n",
    "    return MCPTool(\n",
    "        name=mcp_tool.name,\n",
    "        description=mcp_tool.description or f\"Tool: {mcp_tool.name}\",\n",
    "        args_schema=args_schema,\n",
    "        server_url=server_url,\n",
    "        ws=ws,\n",
    "        is_custom=is_custom,\n",
    "    )\n",
    "\n",
    "\n",
    "# Gather all tools from managed and custom MCP servers into a single list\n",
    "async def create_mcp_tools(\n",
    "    ws: WorkspaceClient, managed_server_urls: List[str] = None, custom_server_urls: List[str] = None\n",
    ") -> List[MCPTool]:\n",
    "    \"\"\"Create LangChain tools from both managed and custom MCP servers\"\"\"\n",
    "    tools = []\n",
    "\n",
    "    if managed_server_urls:\n",
    "        # Load managed MCP tools\n",
    "        for server_url in managed_server_urls:\n",
    "            try:\n",
    "                mcp_tools = get_managed_mcp_tools(ws, server_url)\n",
    "                for mcp_tool in mcp_tools:\n",
    "                    tool = create_langchain_tool_from_mcp(mcp_tool, server_url, ws, is_custom=False)\n",
    "                    tools.append(tool)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading tools from managed server {server_url}: {e}\")\n",
    "\n",
    "    if custom_server_urls:\n",
    "        # Load custom MCP tools (async)\n",
    "        for server_url in custom_server_urls:\n",
    "            try:\n",
    "                mcp_tools = await get_custom_mcp_tools(ws, server_url)\n",
    "                for mcp_tool in mcp_tools:\n",
    "                    tool = create_langchain_tool_from_mcp(mcp_tool, server_url, ws, is_custom=True)\n",
    "                    tools.append(tool)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading tools from custom server {server_url}: {e}\")\n",
    "\n",
    "    return tools\n",
    "\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "\n",
    "# The state for the agent workflow, including the conversation and any custom data\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]\n",
    "    custom_inputs: Optional[dict[str, Any]]\n",
    "    custom_outputs: Optional[dict[str, Any]]\n",
    "\n",
    "\n",
    "# Define the LangGraph agent that can call tools\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[ToolNode, Sequence[BaseTool]],\n",
    "    system_prompt: Optional[str] = None,\n",
    "):\n",
    "    model = model.bind_tools(tools)  # Bind tools to the model\n",
    "\n",
    "    # Function to check if agent should continue or finish based on last message\n",
    "    def should_continue(state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If function (tool) calls are present, continue; otherwise, end\n",
    "        if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    # Preprocess: optionally prepend a system prompt to the conversation history\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "\n",
    "    model_runnable = preprocessor | model  # Chain the preprocessor and the model\n",
    "\n",
    "    # The function to invoke the model within the workflow\n",
    "    def call_model(\n",
    "        state: AgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(AgentState)  # Create the agent's state machine\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))  # Agent node (LLM)\n",
    "    workflow.add_node(\"tools\", ToolNode(tools))  # Tools node\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")  # Start at agent node\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",  # If the model requests a tool call, move to tools node\n",
    "            \"end\": END,  # Otherwise, end the workflow\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")  # After tools are called, return to agent node\n",
    "\n",
    "    # Compile and return the tool-calling agent workflow\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "# ResponsesAgent class to wrap the compiled agent and make it compatible with Mosaic AI Responses API\n",
    "class LangGraphResponsesAgent(ResponsesAgent):\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    # Make a prediction (single-step) for the agent\n",
    "    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:\n",
    "        outputs = [\n",
    "            event.item\n",
    "            for event in self.predict_stream(request)\n",
    "            if event.type == \"response.output_item.done\" or event.type == \"error\"\n",
    "        ]\n",
    "        return ResponsesAgentResponse(output=outputs, custom_outputs=request.custom_inputs)\n",
    "\n",
    "    # Stream predictions for the agent, yielding output as it's generated\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        request: ResponsesAgentRequest,\n",
    "    ) -> Generator[ResponsesAgentStreamEvent, None, None]:\n",
    "        cc_msgs = to_chat_completions_input([i.model_dump() for i in request.input])\n",
    "        # Stream events from the agent graph\n",
    "        for event in self.agent.stream({\"messages\": cc_msgs}, stream_mode=[\"updates\", \"messages\"]):\n",
    "            if event[0] == \"updates\":\n",
    "                # Stream updated messages from the workflow nodes\n",
    "                for node_data in event[1].values():\n",
    "                    if len(node_data.get(\"messages\", [])) > 0:\n",
    "                        yield from output_to_responses_items_stream(node_data[\"messages\"])\n",
    "            elif event[0] == \"messages\":\n",
    "                # Stream generated text message chunks\n",
    "                try:\n",
    "                    chunk = event[1][0]\n",
    "                    if isinstance(chunk, AIMessageChunk) and (content := chunk.content):\n",
    "                        yield ResponsesAgentStreamEvent(\n",
    "                            **self.create_text_delta(delta=content, item_id=chunk.id),\n",
    "                        )\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "# Initialize the entire agent, including MCP tools and workflow\n",
    "def initialize_agent():\n",
    "    \"\"\"Initialize the agent with MCP tools\"\"\"\n",
    "    # Create MCP tools from the configured servers\n",
    "    mcp_tools = asyncio.run(\n",
    "        create_mcp_tools(\n",
    "            ws=workspace_client,\n",
    "            managed_server_urls=MANAGED_MCP_SERVER_URLS,\n",
    "            custom_server_urls=CUSTOM_MCP_SERVER_URLS,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the agent graph with an LLM, tool set, and system prompt (if given)\n",
    "    agent = create_tool_calling_agent(llm, mcp_tools, system_prompt)\n",
    "    return LangGraphResponsesAgent(agent)\n",
    "\n",
    "\n",
    "# Configure MLflow for Databricks deployment (optional for local testing)\n",
    "# Only enable if you want to track runs to Databricks workspace\n",
    "def setup_mlflow():\n",
    "    \"\"\"Setup MLflow tracking and model registration\"\"\"\n",
    "    try:\n",
    "        mlflow.langchain.autolog()\n",
    "        print(\"✓ MLflow autologging enabled\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: MLflow autologging failed: {e}\")\n",
    "\n",
    "# Initialize agent\n",
    "AGENT = initialize_agent()\n",
    "\n",
    "# Try to set up MLflow model tracking (optional)\n",
    "try:\n",
    "    setup_mlflow()\n",
    "    mlflow.models.set_model(AGENT)\n",
    "except Exception as e:\n",
    "    print(f\"Note: MLflow model tracking not available: {e}\")\n",
    "    print(\"Agent will work without MLflow tracking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9633415f",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "## Step 2: Test the Agent\n",
    "\n",
    "Load and test the agent with sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a851822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MLflow tracking configured: databricks://development\n",
      "✓ Experiment set to: /Users/huy.d@hotmail.com/langgraph-mcp-agent\n",
      "✓ Experiment set to: /Users/huy.d@hotmail.com/langgraph-mcp-agent\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# Configure MLflow tracking BEFORE importing agent\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "profile_name = os.getenv(\"DATABRICKS_CONFIG_PROFILE\", \"development\")\n",
    "\n",
    "try:\n",
    "    # Set tracking URI with profile\n",
    "    mlflow.set_tracking_uri(f\"databricks://{profile_name}\")\n",
    "    print(f\"✓ MLflow tracking configured: databricks://{profile_name}\")\n",
    "    \n",
    "    # Set experiment name\n",
    "    experiment_name = \"/Users/huy.d@hotmail.com/langgraph-mcp-agent\"  # TODO: Update with your email\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\"✓ Experiment set to: {experiment_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: MLflow tracking not configured: {e}\")\n",
    "    print(\"Agent will work without MLflow tracking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62b6a109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MLflow autologging enabled\n",
      "tool_choice=None truncation=None id=None created_at=None error=None incomplete_details=None instructions=None metadata=None model=None object='response' output=[OutputItem(type='message', id='lc_run--57685e74-4a60-4850-840a-7a399f7fb2c8', content=[{'text': 'I can calculate 7*6 using Python. Let me run that for you:', 'type': 'output_text'}], role='assistant'), OutputItem(type='function_call', id='lc_run--57685e74-4a60-4850-840a-7a399f7fb2c8', call_id='toolu_bdrk_01RtVY3SpWi14fWqPDcVFefM', name='system__ai__python_exec', arguments='{\"code\": \"print(7 * 6)\"}'), OutputItem(type='function_call_output', call_id='toolu_bdrk_01RtVY3SpWi14fWqPDcVFefM', output='{\"is_truncated\":false,\"columns\":[\"output\"],\"rows\":[[\"42\\\\n\"]]}'), OutputItem(type='message', id='lc_run--dd34d21c-ea34-4c6d-b0c2-a20724ddd01b', content=[{'text': 'The result of 7 * 6 in Python is 42.', 'type': 'output_text'}], role='assistant')] parallel_tool_calls=None temperature=None tools=None top_p=None max_output_tokens=None previous_response_id=None reasoning=None status=None text=None usage=None user=None custom_outputs=None\n",
      "tool_choice=None truncation=None id=None created_at=None error=None incomplete_details=None instructions=None metadata=None model=None object='response' output=[OutputItem(type='message', id='lc_run--57685e74-4a60-4850-840a-7a399f7fb2c8', content=[{'text': 'I can calculate 7*6 using Python. Let me run that for you:', 'type': 'output_text'}], role='assistant'), OutputItem(type='function_call', id='lc_run--57685e74-4a60-4850-840a-7a399f7fb2c8', call_id='toolu_bdrk_01RtVY3SpWi14fWqPDcVFefM', name='system__ai__python_exec', arguments='{\"code\": \"print(7 * 6)\"}'), OutputItem(type='function_call_output', call_id='toolu_bdrk_01RtVY3SpWi14fWqPDcVFefM', output='{\"is_truncated\":false,\"columns\":[\"output\"],\"rows\":[[\"42\\\\n\"]]}'), OutputItem(type='message', id='lc_run--dd34d21c-ea34-4c6d-b0c2-a20724ddd01b', content=[{'text': 'The result of 7 * 6 in Python is 42.', 'type': 'output_text'}], role='assistant')] parallel_tool_calls=None temperature=None tools=None top_p=None max_output_tokens=None previous_response_id=None reasoning=None status=None text=None usage=None user=None custom_outputs=None\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from agent import AGENT\n",
    "\n",
    "# Test the agent with a simple query\n",
    "response = AGENT.predict({\n",
    "    \"input\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is 7*6 in Python?\"}\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6d347e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_choice=None truncation=None id=None created_at=None error=None incomplete_details=None instructions=None metadata=None model=None object='response' output=[OutputItem(type='message', id='lc_run--57685e74-4a60-4850-840a-7a399f7fb2c8', content=[{'text': 'I can calculate 7*6 using Python. Let me run that for you:', 'type': 'output_text'}], role='assistant'), OutputItem(type='function_call', id='lc_run--57685e74-4a60-4850-840a-7a399f7fb2c8', call_id='toolu_bdrk_01RtVY3SpWi14fWqPDcVFefM', name='system__ai__python_exec', arguments='{\"code\": \"print(7 * 6)\"}'), OutputItem(type='function_call_output', call_id='toolu_bdrk_01RtVY3SpWi14fWqPDcVFefM', output='{\"is_truncated\":false,\"columns\":[\"output\"],\"rows\":[[\"42\\\\n\"]]}'), OutputItem(type='message', id='lc_run--dd34d21c-ea34-4c6d-b0c2-a20724ddd01b', content=[{'text': 'The result of 7 * 6 in Python is 42.', 'type': 'output_text'}], role='assistant')] parallel_tool_calls=None temperature=None tools=None top_p=None max_output_tokens=None previous_response_id=None reasoning=None status=None text=None usage=None user=None custom_outputs=None\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--51af6dd5-f900-4925-8c72-18186250c3f8' delta=\"I'll calculate the\" -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--51af6dd5-f900-4925-8c72-18186250c3f8' delta=' 15th' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--51af6dd5-f900-4925-8c72-18186250c3f8' delta=' Fibonacci number using Python' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--51af6dd5-f900-4925-8c72-18186250c3f8' delta='.' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--51af6dd5-f900-4925-8c72-18186250c3f8' delta=\"I'll calculate the\" -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--51af6dd5-f900-4925-8c72-18186250c3f8' delta=' 15th' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--51af6dd5-f900-4925-8c72-18186250c3f8' delta=' Fibonacci number using Python' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--51af6dd5-f900-4925-8c72-18186250c3f8' delta='.' -----------\n",
      "\n",
      "type='response.output_item.done' custom_outputs=None item={'id': 'lc_run--51af6dd5-f900-4925-8c72-18186250c3f8', 'content': [{'text': \"I'll calculate the 15th Fibonacci number using Python.\", 'type': 'output_text'}], 'role': 'assistant', 'type': 'message'} -----------\n",
      "\n",
      "type='response.output_item.done' custom_outputs=None item={'type': 'function_call', 'id': 'lc_run--51af6dd5-f900-4925-8c72-18186250c3f8', 'call_id': 'toolu_bdrk_01EXpkh9J9P7v4h3Z8dJd6x3', 'name': 'system__ai__python_exec', 'arguments': '{\"code\": \"# Function to calculate Fibonacci numbers\\\\ndef fibonacci(n):\\\\n    if n <= 0:\\\\n        return \\\\\"Please enter a positive integer\\\\\"\\\\n    elif n == 1:\\\\n        return 0\\\\n    elif n == 2:\\\\n        return 1\\\\n    else:\\\\n        a, b = 0, 1\\\\n        for i in range(3, n + 1):\\\\n            a, b = b, a + b\\\\n        return b\\\\n\\\\n# Calculate the 15th Fibonacci number\\\\nfib_15 = fibonacci(15)\\\\nprint(f\\\\\"The 15th Fibonacci number is: {fib_15}\\\\\")\"}'} -----------\n",
      "\n",
      "type='response.output_item.done' custom_outputs=None item={'id': 'lc_run--51af6dd5-f900-4925-8c72-18186250c3f8', 'content': [{'text': \"I'll calculate the 15th Fibonacci number using Python.\", 'type': 'output_text'}], 'role': 'assistant', 'type': 'message'} -----------\n",
      "\n",
      "type='response.output_item.done' custom_outputs=None item={'type': 'function_call', 'id': 'lc_run--51af6dd5-f900-4925-8c72-18186250c3f8', 'call_id': 'toolu_bdrk_01EXpkh9J9P7v4h3Z8dJd6x3', 'name': 'system__ai__python_exec', 'arguments': '{\"code\": \"# Function to calculate Fibonacci numbers\\\\ndef fibonacci(n):\\\\n    if n <= 0:\\\\n        return \\\\\"Please enter a positive integer\\\\\"\\\\n    elif n == 1:\\\\n        return 0\\\\n    elif n == 2:\\\\n        return 1\\\\n    else:\\\\n        a, b = 0, 1\\\\n        for i in range(3, n + 1):\\\\n            a, b = b, a + b\\\\n        return b\\\\n\\\\n# Calculate the 15th Fibonacci number\\\\nfib_15 = fibonacci(15)\\\\nprint(f\\\\\"The 15th Fibonacci number is: {fib_15}\\\\\")\"}'} -----------\n",
      "\n",
      "type='response.output_item.done' custom_outputs=None item={'type': 'function_call_output', 'call_id': 'toolu_bdrk_01EXpkh9J9P7v4h3Z8dJd6x3', 'output': '{\"is_truncated\":false,\"columns\":[\"output\"],\"rows\":[[\"The 15th Fibonacci number is: 377\\\\n\"]]}'} -----------\n",
      "\n",
      "type='response.output_item.done' custom_outputs=None item={'type': 'function_call_output', 'call_id': 'toolu_bdrk_01EXpkh9J9P7v4h3Z8dJd6x3', 'output': '{\"is_truncated\":false,\"columns\":[\"output\"],\"rows\":[[\"The 15th Fibonacci number is: 377\\\\n\"]]}'} -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='The 15' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='th Fibonacci number' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' is ' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='377.\\n\\nI' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' calculated this using a' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' Python function that computes' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='The 15' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='th Fibonacci number' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' is ' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='377.\\n\\nI' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' calculated this using a' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' Python function that computes' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' Fibonacci numbers iteratively' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='. The Fibonacci sequence starts with' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' 0 an' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='d 1, an' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='d each subsequent number is' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' Fibonacci numbers iteratively' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='. The Fibonacci sequence starts with' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' 0 an' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='d 1, an' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='d each subsequent number is' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' the sum of the' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' two preceding ones' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='. The function' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' tracks two consecutive' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' the sum of the' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' two preceding ones' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='. The function' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' tracks two consecutive' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' Fibonacci numbers an' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='d updates them in' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' each iteration until' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' reaching the desire' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' Fibonacci numbers an' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='d updates them in' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' each iteration until' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' reaching the desire' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='d position in' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' the sequence.' -----------\n",
      "\n",
      "type='response.output_item.done' custom_outputs=None item={'id': 'lc_run--21294410-4652-4dbc-bb10-769361332b16', 'content': [{'text': 'The 15th Fibonacci number is 377.\\n\\nI calculated this using a Python function that computes Fibonacci numbers iteratively. The Fibonacci sequence starts with 0 and 1, and each subsequent number is the sum of the two preceding ones. The function tracks two consecutive Fibonacci numbers and updates them in each iteration until reaching the desired position in the sequence.', 'type': 'output_text'}], 'role': 'assistant', 'type': 'message'} -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta='d position in' -----------\n",
      "\n",
      "type='response.output_text.delta' custom_outputs=None item_id='lc_run--21294410-4652-4dbc-bb10-769361332b16' delta=' the sequence.' -----------\n",
      "\n",
      "type='response.output_item.done' custom_outputs=None item={'id': 'lc_run--21294410-4652-4dbc-bb10-769361332b16', 'content': [{'text': 'The 15th Fibonacci number is 377.\\n\\nI calculated this using a Python function that computes Fibonacci numbers iteratively. The Fibonacci sequence starts with 0 and 1, and each subsequent number is the sum of the two preceding ones. The function tracks two consecutive Fibonacci numbers and updates them in each iteration until reaching the desired position in the sequence.', 'type': 'output_text'}], 'role': 'assistant', 'type': 'message'} -----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "print(response)\n",
    "for chunk in AGENT.predict_stream({\n",
    "    \"input\": [\n",
    "        {\"role\": \"user\", \"content\": \"Calculate the 15th Fibonacci number using Python\"}\n",
    "    ]\n",
    "}):\n",
    "    print(chunk, \"-----------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b0c9a",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "## Step 3: Evaluate the Agent\n",
    "\n",
    "Run evaluation with predefined LLM scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18c238e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/08 22:44:33 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n",
      "2025/11/08 22:44:33 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n",
      "2025/11/08 22:44:33 WARNING mlflow.tracing.fluent: Failed to start span predict_stream: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "2025/11/08 22:44:33 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "2025/11/08 22:44:33 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n",
      "2025/11/08 22:44:33 WARNING mlflow.tracing.fluent: Failed to start span predict_stream: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "2025/11/08 22:44:33 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "{\"ts\": \"2025-11-08 22:44:40,359\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\\\", line 2036, in config\", \"    resp = self._stub.Config(req, metadata=self.metadata())\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 276, in __call__\", \"    response, ignored_call = self._with_call(\", \"                             ^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 331, in _with_call\", \"    return call.result(), call\", \"           ^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 438, in result\", \"    raise self\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 314, in continuation\", \"    response, call = self._thunk(new_method).with_call(\", \"                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 1180, in with_call\", \"    return _end_unary_response_blocking(state, call, True, None)\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 996, in _end_unary_response_blocking\", \"    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\", \"    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"}\\\"\", \">\"]}}\n",
      "{\"ts\": \"2025-11-08 22:44:40,359\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\\\", line 2036, in config\", \"    resp = self._stub.Config(req, metadata=self.metadata())\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 276, in __call__\", \"    response, ignored_call = self._with_call(\", \"                             ^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 331, in _with_call\", \"    return call.result(), call\", \"           ^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 438, in result\", \"    raise self\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 314, in continuation\", \"    response, call = self._thunk(new_method).with_call(\", \"                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 1180, in with_call\", \"    return _end_unary_response_blocking(state, call, True, None)\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 996, in _end_unary_response_blocking\", \"    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\", \"    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"}\\\"\", \">\"]}}\n",
      "ERROR:pyspark.sql.connect.client.logging:GRPC Error received\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 2036, in config\n",
      "    resp = self._stub.Config(req, metadata=self.metadata())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 276, in __call__\n",
      "    response, ignored_call = self._with_call(\n",
      "                             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 331, in _with_call\n",
      "    return call.result(), call\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 438, in result\n",
      "    raise self\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 314, in continuation\n",
      "    response, call = self._thunk(new_method).with_call(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 1180, in with_call\n",
      "    return _end_unary_response_blocking(state, call, True, None)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 996, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.INTERNAL\n",
      "\tdetails = \"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\"}\"\n",
      ">\n",
      "{\"ts\": \"2025-11-08 22:44:40,359\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\\\", line 2036, in config\", \"    resp = self._stub.Config(req, metadata=self.metadata())\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 276, in __call__\", \"    response, ignored_call = self._with_call(\", \"                             ^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 331, in _with_call\", \"    return call.result(), call\", \"           ^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 438, in result\", \"    raise self\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 314, in continuation\", \"    response, call = self._thunk(new_method).with_call(\", \"                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 1180, in with_call\", \"    return _end_unary_response_blocking(state, call, True, None)\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 996, in _end_unary_response_blocking\", \"    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\", \"    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"}\\\"\", \">\"]}}\n",
      "{\"ts\": \"2025-11-08 22:44:40,359\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\\\", line 2036, in config\", \"    resp = self._stub.Config(req, metadata=self.metadata())\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 276, in __call__\", \"    response, ignored_call = self._with_call(\", \"                             ^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 331, in _with_call\", \"    return call.result(), call\", \"           ^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 438, in result\", \"    raise self\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 314, in continuation\", \"    response, call = self._thunk(new_method).with_call(\", \"                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 1180, in with_call\", \"    return _end_unary_response_blocking(state, call, True, None)\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 996, in _end_unary_response_blocking\", \"    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\", \"    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"}\\\"\", \">\"]}}\n",
      "ERROR:pyspark.sql.connect.client.logging:GRPC Error received\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 2036, in config\n",
      "    resp = self._stub.Config(req, metadata=self.metadata())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 276, in __call__\n",
      "    response, ignored_call = self._with_call(\n",
      "                             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 331, in _with_call\n",
      "    return call.result(), call\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 438, in result\n",
      "    raise self\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 314, in continuation\n",
      "    response, call = self._thunk(new_method).with_call(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 1180, in with_call\n",
      "    return _end_unary_response_blocking(state, call, True, None)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 996, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.INTERNAL\n",
      "\tdetails = \"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\"}\"\n",
      ">\n",
      "{\"ts\": \"2025-11-08 22:44:40,884\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\\\", line 2036, in config\", \"    resp = self._stub.Config(req, metadata=self.metadata())\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 276, in __call__\", \"    response, ignored_call = self._with_call(\", \"                             ^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 331, in _with_call\", \"    return call.result(), call\", \"           ^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 438, in result\", \"    raise self\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 314, in continuation\", \"    response, call = self._thunk(new_method).with_call(\", \"                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 1180, in with_call\", \"    return _end_unary_response_blocking(state, call, True, None)\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 996, in _end_unary_response_blocking\", \"    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\", \"    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\", \">\"]}}\n",
      "{\"ts\": \"2025-11-08 22:44:40,884\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\\\", line 2036, in config\", \"    resp = self._stub.Config(req, metadata=self.metadata())\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 276, in __call__\", \"    response, ignored_call = self._with_call(\", \"                             ^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 331, in _with_call\", \"    return call.result(), call\", \"           ^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 438, in result\", \"    raise self\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 314, in continuation\", \"    response, call = self._thunk(new_method).with_call(\", \"                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 1180, in with_call\", \"    return _end_unary_response_blocking(state, call, True, None)\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 996, in _end_unary_response_blocking\", \"    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\", \"    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\", \">\"]}}\n",
      "ERROR:pyspark.sql.connect.client.logging:GRPC Error received\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 2036, in config\n",
      "    resp = self._stub.Config(req, metadata=self.metadata())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 276, in __call__\n",
      "    response, ignored_call = self._with_call(\n",
      "                             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 331, in _with_call\n",
      "    return call.result(), call\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 438, in result\n",
      "    raise self\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 314, in continuation\n",
      "    response, call = self._thunk(new_method).with_call(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 1180, in with_call\n",
      "    return _end_unary_response_blocking(state, call, True, None)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 996, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.INTERNAL\n",
      "\tdetails = \"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\", grpc_status:13}\"\n",
      ">\n",
      "{\"ts\": \"2025-11-08 22:44:40,884\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\\\", line 2036, in config\", \"    resp = self._stub.Config(req, metadata=self.metadata())\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 276, in __call__\", \"    response, ignored_call = self._with_call(\", \"                             ^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 331, in _with_call\", \"    return call.result(), call\", \"           ^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 438, in result\", \"    raise self\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 314, in continuation\", \"    response, call = self._thunk(new_method).with_call(\", \"                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 1180, in with_call\", \"    return _end_unary_response_blocking(state, call, True, None)\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 996, in _end_unary_response_blocking\", \"    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\", \"    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\", \">\"]}}\n",
      "{\"ts\": \"2025-11-08 22:44:40,884\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\\\", line 2036, in config\", \"    resp = self._stub.Config(req, metadata=self.metadata())\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 276, in __call__\", \"    response, ignored_call = self._with_call(\", \"                             ^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 331, in _with_call\", \"    return call.result(), call\", \"           ^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 438, in result\", \"    raise self\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 314, in continuation\", \"    response, call = self._thunk(new_method).with_call(\", \"                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 1180, in with_call\", \"    return _end_unary_response_blocking(state, call, True, None)\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 996, in _end_unary_response_blocking\", \"    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\", \"    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\", \">\"]}}\n",
      "ERROR:pyspark.sql.connect.client.logging:GRPC Error received\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 2036, in config\n",
      "    resp = self._stub.Config(req, metadata=self.metadata())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 276, in __call__\n",
      "    response, ignored_call = self._with_call(\n",
      "                             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 331, in _with_call\n",
      "    return call.result(), call\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 438, in result\n",
      "    raise self\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 314, in continuation\n",
      "    response, call = self._thunk(new_method).with_call(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 1180, in with_call\n",
      "    return _end_unary_response_blocking(state, call, True, None)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 996, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.INTERNAL\n",
      "\tdetails = \"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\", grpc_status:13}\"\n",
      ">\n",
      "{\"ts\": \"2025-11-08 22:44:41,239\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\\\", line 2036, in config\", \"    resp = self._stub.Config(req, metadata=self.metadata())\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 276, in __call__\", \"    response, ignored_call = self._with_call(\", \"                             ^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 331, in _with_call\", \"    return call.result(), call\", \"           ^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 438, in result\", \"    raise self\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 314, in continuation\", \"    response, call = self._thunk(new_method).with_call(\", \"                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 1180, in with_call\", \"    return _end_unary_response_blocking(state, call, True, None)\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 996, in _end_unary_response_blocking\", \"    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\", \"    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\", \">\"]}}\n",
      "{\"ts\": \"2025-11-08 22:44:41,239\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\\\", line 2036, in config\", \"    resp = self._stub.Config(req, metadata=self.metadata())\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 276, in __call__\", \"    response, ignored_call = self._with_call(\", \"                             ^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 331, in _with_call\", \"    return call.result(), call\", \"           ^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 438, in result\", \"    raise self\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 314, in continuation\", \"    response, call = self._thunk(new_method).with_call(\", \"                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 1180, in with_call\", \"    return _end_unary_response_blocking(state, call, True, None)\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 996, in _end_unary_response_blocking\", \"    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\", \"    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\", \">\"]}}\n",
      "ERROR:pyspark.sql.connect.client.logging:GRPC Error received\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 2036, in config\n",
      "    resp = self._stub.Config(req, metadata=self.metadata())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 276, in __call__\n",
      "    response, ignored_call = self._with_call(\n",
      "                             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 331, in _with_call\n",
      "    return call.result(), call\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 438, in result\n",
      "    raise self\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 314, in continuation\n",
      "    response, call = self._thunk(new_method).with_call(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 1180, in with_call\n",
      "    return _end_unary_response_blocking(state, call, True, None)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 996, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.INTERNAL\n",
      "\tdetails = \"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\", grpc_status:13}\"\n",
      ">\n",
      "{\"ts\": \"2025-11-08 22:44:41,239\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\\\", line 2036, in config\", \"    resp = self._stub.Config(req, metadata=self.metadata())\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 276, in __call__\", \"    response, ignored_call = self._with_call(\", \"                             ^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 331, in _with_call\", \"    return call.result(), call\", \"           ^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 438, in result\", \"    raise self\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 314, in continuation\", \"    response, call = self._thunk(new_method).with_call(\", \"                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 1180, in with_call\", \"    return _end_unary_response_blocking(state, call, True, None)\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 996, in _end_unary_response_blocking\", \"    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\", \"    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\", \">\"]}}\n",
      "{\"ts\": \"2025-11-08 22:44:41,239\", \"level\": \"ERROR\", \"logger\": \"pyspark.sql.connect.client.logging\", \"msg\": \"GRPC Error received\", \"context\": {}, \"exception\": {\"class\": \"_InactiveRpcError\", \"msg\": \"<_InactiveRpcError of RPC that terminated with:\\n\\tstatus = StatusCode.INTERNAL\\n\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\\n\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\\n>\", \"stacktrace\": [\"Traceback (most recent call last):\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\\\", line 2036, in config\", \"    resp = self._stub.Config(req, metadata=self.metadata())\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 276, in __call__\", \"    response, ignored_call = self._with_call(\", \"                             ^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 331, in _with_call\", \"    return call.result(), call\", \"           ^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 438, in result\", \"    raise self\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\\\", line 314, in continuation\", \"    response, call = self._thunk(new_method).with_call(\", \"                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 1180, in with_call\", \"    return _end_unary_response_blocking(state, call, True, None)\", \"           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"  File \\\"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\\\", line 996, in _end_unary_response_blocking\", \"    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\", \"    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\", \"grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\", \"\\tstatus = StatusCode.INTERNAL\", \"\\tdetails = \\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\"\", \"\\tdebug_error_string = \\\"UNKNOWN:Error received from peer  {grpc_message:\\\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\\\", grpc_status:13}\\\"\", \">\"]}}\n",
      "ERROR:pyspark.sql.connect.client.logging:GRPC Error received\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py\", line 2036, in config\n",
      "    resp = self._stub.Config(req, metadata=self.metadata())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 276, in __call__\n",
      "    response, ignored_call = self._with_call(\n",
      "                             ^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 331, in _with_call\n",
      "    return call.result(), call\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 438, in result\n",
      "    raise self\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_interceptor.py\", line 314, in continuation\n",
      "    response, call = self._thunk(new_method).with_call(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 1180, in with_call\n",
      "    return _end_unary_response_blocking(state, call, True, None)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/grpc/_channel.py\", line 996, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)  # pytype: disable=not-instantiable\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.INTERNAL\n",
      "\tdetails = \"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\"\n",
      "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"[CONFIG_NOT_AVAILABLE] Configuration spark.mlflow.modelRegistryUri is not available. SQLSTATE: 42K0I\", grpc_status:13}\"\n",
      ">\n",
      "/Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Evaluating:   0%|          | 0/1 [Elapsed: 00:00, Remaining: ?] /Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Evaluating: 100%|██████████| 1/1 [Elapsed: 00:10, Remaining: 00:00] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"https://adb-3253299566947192.12.azuredatabricks.net/ml/experiments/897801252548174/evaluation-runs?selectedRunUuid=676bf18826ad43c089e85b08f1c461f9\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation complete. Check the MLflow UI for detailed results.\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "import mlflow\n",
    "from mlflow.genai.scorers import RelevanceToQuery, Safety\n",
    "\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"input\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Calculate the 15th Fibonacci number\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        \"expected_response\": \"The 15th Fibonacci number is 610.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=lambda input: AGENT.predict({\"input\": input}),\n",
    "    scorers=[RelevanceToQuery(), Safety()],\n",
    ")\n",
    "\n",
    "# Review the evaluation results in the MLflow UI\n",
    "print(\"Evaluation complete. Check the MLflow UI for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626a7ffd",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "## Step 4: Log the Agent as an MLflow Model\n",
    "\n",
    "Log the agent code for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14731fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MLflow tracking configured with profile: development\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gg/dkwbth_n51q5568p928r8r_m0000gn/T/ipykernel_82190/1277033277.py:5: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Experiment set to: /Users/huy.d@hotmail.com/langgraph-mcp-agent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🔗 View Logged Model at: https://adb-3253299566947192.12.azuredatabricks.net/ml/experiments/897801252548174/models/m-917a987360744911bce572bc7e4c090c\n",
      "2025/11/08 22:44:55 INFO mlflow.pyfunc: Predicting on input example to validate output\n",
      "2025/11/08 22:44:55 WARNING mlflow.tracing.fluent: Failed to start span predict_stream: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "2025/11/08 22:44:55 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "2025/11/08 22:44:55 INFO mlflow.pyfunc: Predicting on input example to validate output\n",
      "2025/11/08 22:44:55 WARNING mlflow.tracing.fluent: Failed to start span predict_stream: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "2025/11/08 22:44:55 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MLflow autologging enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/08 22:44:58 WARNING mlflow.tracing.fluent: Failed to start span predict_stream: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "2025/11/08 22:44:58 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "2025/11/08 22:44:58 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MLflow autologging enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/08 22:45:00 INFO mlflow.models.model: Found the following environment variables used during model inference: [DATABRICKS_HOST]. Please check if you need to set them when deploying the model. To disable this message, set environment variable `MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING` to `false`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run adorable-goat-953 at: https://adb-3253299566947192.12.azuredatabricks.net/ml/experiments/897801252548174/runs/a8dd041ca3b14042ac50467dbf51b15b\n",
      "🧪 View experiment at: https://adb-3253299566947192.12.azuredatabricks.net/ml/experiments/897801252548174\n",
      "Model logged: models:/m-917a987360744911bce572bc7e4c090c\n",
      "Model logged: models:/m-917a987360744911bce572bc7e4c090c\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "import mlflow\n",
    "import os\n",
    "from agent import LLM_ENDPOINT_NAME\n",
    "from mlflow.models.resources import DatabricksServingEndpoint, DatabricksFunction\n",
    "from pkg_resources import get_distribution\n",
    "\n",
    "# Set tracking URI to Databricks workspace with profile\n",
    "# MLflow supports 'databricks://{profile}' format for multiple profiles\n",
    "profile_name = os.getenv(\"DATABRICKS_CONFIG_PROFILE\", \"development\")\n",
    "\n",
    "try:\n",
    "    # Use profile-specific tracking URI\n",
    "    mlflow.set_tracking_uri(f\"databricks://{profile_name}\")\n",
    "    print(f\"✓ MLflow tracking configured with profile: {profile_name}\")\n",
    "    \n",
    "    # Set experiment name\n",
    "    experiment_name = \"/Users/huy.d@hotmail.com/langgraph-mcp-agent\"  # TODO: Update with your email\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    print(f\"✓ Experiment set to: {experiment_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not set tracking URI: {e}\")\n",
    "    print(\"Continuing with default tracking URI...\")\n",
    "\n",
    "resources = [\n",
    "    DatabricksServingEndpoint(endpoint_name=LLM_ENDPOINT_NAME), \n",
    "    DatabricksFunction(function_name=\"system.ai.python_exec\")\n",
    "]\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "        name=\"agent\",\n",
    "        python_model=\"agent.py\",\n",
    "        resources=resources,\n",
    "        pip_requirements=[\n",
    "            \"databricks-mcp\",\n",
    "            f\"langgraph=={get_distribution('langgraph').version}\",\n",
    "            f\"mcp=={get_distribution('mcp').version}\",\n",
    "            f\"databricks-langchain=={get_distribution('databricks-langchain').version}\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(f\"Model logged: {logged_agent_info.model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b59d76c",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "## Step 5: Pre-deployment Validation\n",
    "\n",
    "Validate the model before deploying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbc0243b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]WARNING:databricks.sdk.mixins.files:Unsuccessful download. Response status: 404, body: b'\\xef\\xbb\\xbf<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\\nRequestId:b9e3a12c-f01e-003a-4f2b-5127b9000000\\nTime:2025-11-09T03:45:03.5761152Z</Message></Error>'\n",
      "WARNING:databricks.sdk.mixins.files:Unsuccessful download. Response status: 404, body: b'\\xef\\xbb\\xbf<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\\nRequestId:b9e3a12c-f01e-003a-4f2b-5127b9000000\\nTime:2025-11-09T03:45:03.5761152Z</Message></Error>'\n",
      "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Downloading artifacts: 100%|██████████| 9/9 [00:00<00:00, 33.28it/s]   \n",
      "2025/11/08 22:45:05 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "Downloading artifacts: 100%|██████████| 9/9 [00:00<00:00, 33.28it/s]\n",
      "2025/11/08 22:45:05 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]WARNING:databricks.sdk.mixins.files:Unsuccessful download. Response status: 404, body: b'\\xef\\xbb\\xbf<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\\nRequestId:fc7243a1-601e-0081-372b-519d1b000000\\nTime:2025-11-09T03:45:05.7146710Z</Message></Error>'\n",
      "WARNING:databricks.sdk.mixins.files:Unsuccessful download. Response status: 404, body: b'\\xef\\xbb\\xbf<?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\\nRequestId:fc7243a1-601e-0081-372b-519d1b000000\\nTime:2025-11-09T03:45:05.7146710Z</Message></Error>'\n",
      "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "Downloading artifacts: 100%|██████████| 9/9 [00:00<00:00, 35.22it/s]   \n",
      "2025/11/08 22:45:08 INFO mlflow.utils.virtualenv: Environment /tmp/virtualenv_envs/mlflow-fe38911592d12d9b07553a1cb2d44173ca6747d4 already exists\n",
      "2025/11/08 22:45:08 WARNING mlflow.utils.databricks_utils: Missing required environment variable `SPARK_LOCAL_REMOTE` or `SPARK_REMOTE`. These are necessary to initialize the WorkspaceClient with serverless compute in a subprocess in Databricks for UC function execution. Setting the value to 'true'.\n",
      "2025/11/08 22:45:08 INFO mlflow.utils.environment: === Running command '['bash', '-c', 'source /tmp/virtualenv_envs/mlflow-fe38911592d12d9b07553a1cb2d44173ca6747d4/bin/activate && python -c \"\"']'\n",
      "\n",
      "2025/11/08 22:45:08 INFO mlflow.utils.virtualenv: Environment /tmp/virtualenv_envs/mlflow-fe38911592d12d9b07553a1cb2d44173ca6747d4 already exists\n",
      "2025/11/08 22:45:08 WARNING mlflow.utils.databricks_utils: Missing required environment variable `SPARK_LOCAL_REMOTE` or `SPARK_REMOTE`. These are necessary to initialize the WorkspaceClient with serverless compute in a subprocess in Databricks for UC function execution. Setting the value to 'true'.\n",
      "2025/11/08 22:45:08 INFO mlflow.utils.environment: === Running command '['bash', '-c', 'source /tmp/virtualenv_envs/mlflow-fe38911592d12d9b07553a1cb2d44173ca6747d4/bin/activate && python -c \"\"']'\n",
      "2025/11/08 22:45:08 WARNING mlflow.utils.databricks_utils: Missing required environment variable `SPARK_LOCAL_REMOTE` or `SPARK_REMOTE`. These are necessary to initialize the WorkspaceClient with serverless compute in a subprocess in Databricks for UC function execution. Setting the value to 'true'.\n",
      "2025/11/08 22:45:08 INFO mlflow.utils.environment: === Running command '['bash', '-c', 'source /tmp/virtualenv_envs/mlflow-fe38911592d12d9b07553a1cb2d44173ca6747d4/bin/activate && python /Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py --model-uri file:///var/folders/gg/dkwbth_n51q5568p928r8r_m0000gn/T/tmpzgpe84gf/agent --content-type json --input-path /var/folders/gg/dkwbth_n51q5568p928r8r_m0000gn/T/tmp51h_6273/input.json']'\n",
      "2025/11/08 22:45:08 WARNING mlflow.utils.databricks_utils: Missing required environment variable `SPARK_LOCAL_REMOTE` or `SPARK_REMOTE`. These are necessary to initialize the WorkspaceClient with serverless compute in a subprocess in Databricks for UC function execution. Setting the value to 'true'.\n",
      "2025/11/08 22:45:08 INFO mlflow.utils.environment: === Running command '['bash', '-c', 'source /tmp/virtualenv_envs/mlflow-fe38911592d12d9b07553a1cb2d44173ca6747d4/bin/activate && python /Users/huydo/Projects/Databricks/langgraph/lg-demo/.venv/lib/python3.11/site-packages/mlflow/pyfunc/_mlflow_pyfunc_backend_predict.py --model-uri file:///var/folders/gg/dkwbth_n51q5568p928r8r_m0000gn/T/tmpzgpe84gf/agent --content-type json --input-path /var/folders/gg/dkwbth_n51q5568p928r8r_m0000gn/T/tmp51h_6273/input.json']'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MLflow autologging enabled\n",
      "{\"object\": \"response\", \"output\": [{\"type\": \"message\", \"id\": \"lc_run--80ee1d12-6c02-42ec-97c8-0bc4423ecade\", \"content\": [{\"text\": \"I can calculate 7*6 using Python. Let me run the code for you:\", \"type\": \"output_text\"}], \"role\": \"assistant\"}, {\"type\": \"function_call\", \"id\": \"lc_run--80ee1d12-6c02-42ec-97c8-0bc4423ecade\", \"call_id\": \"toolu_bdrk_01UYADGEiB1ForPtNhJ3fcZN\", \"name\": \"system__ai__python_exec\", \"arguments\": \"{\\\"code\\\": \\\"print(7 * 6)\\\"}\"}, {\"type\": \"function_call_output\", \"call_id\": \"toolu_bdrk_01UYADGEiB1ForPtNhJ3fcZN\", \"output\": \"{\\\"is_truncated\\\":false,\\\"columns\\\":[\\\"output\\\"],\\\"rows\\\":[[\\\"42\\\\n\\\"]]}\"}, {\"type\": \"message\", \"id\": \"lc_run--24ace0f4-336c-4482-8d1d-45c8acbc0c0d\", \"content\": [{\"text\": \"The result of 7*6 in Python is 42.\", \"type\": \"output_text\"}], \"role\": \"assistant\"}]}{\"object\": \"response\", \"output\": [{\"type\": \"message\", \"id\": \"lc_run--80ee1d12-6c02-42ec-97c8-0bc4423ecade\", \"content\": [{\"text\": \"I can calculate 7*6 using Python. Let me run the code for you:\", \"type\": \"output_text\"}], \"role\": \"assistant\"}, {\"type\": \"function_call\", \"id\": \"lc_run--80ee1d12-6c02-42ec-97c8-0bc4423ecade\", \"call_id\": \"toolu_bdrk_01UYADGEiB1ForPtNhJ3fcZN\", \"name\": \"system__ai__python_exec\", \"arguments\": \"{\\\"code\\\": \\\"print(7 * 6)\\\"}\"}, {\"type\": \"function_call_output\", \"call_id\": \"toolu_bdrk_01UYADGEiB1ForPtNhJ3fcZN\", \"output\": \"{\\\"is_truncated\\\":false,\\\"columns\\\":[\\\"output\\\"],\\\"rows\\\":[[\\\"42\\\\n\\\"]]}\"}, {\"type\": \"message\", \"id\": \"lc_run--24ace0f4-336c-4482-8d1d-45c8acbc0c0d\", \"content\": [{\"text\": \"The result of 7*6 in Python is 42.\", \"type\": \"output_text\"}], \"role\": \"assistant\"}]}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/08 22:45:15 INFO mlflow.tracing.export.async_export_queue: Flushing the async trace logging queue before program exit. This may take a while...\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "mlflow.models.predict(\n",
    "    model_uri=f\"runs:/{logged_agent_info.run_id}/agent\",\n",
    "    input_data={\"input\": [{\"role\": \"user\", \"content\": \"What is 7*6 in Python?\"}]},\n",
    "    env_manager=\"uv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b878e6",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "## Step 6: Register to Unity Catalog\n",
    "\n",
    "Register the agent to Unity Catalog before deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaeb787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Registry URI set to: databricks-uc://development\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'rag.development.langgraph_mcp_agent' already exists. Creating a new version of this model...\n",
      "Downloading artifacts: 100%|██████████| 9/9 [00:01<00:00,  7.14it/s]   \n",
      "\n",
      "Uploading artifacts: 100%|██████████| 10/10 [00:00<00:00, 11.10it/s]\n",
      "\n",
      "🔗 Created version '3' of model 'rag.development.langgraph_mcp_agent': https://adb-3253299566947192.12.azuredatabricks.net/explore/data/models/rag/development/langgraph_mcp_agent/version/3\n",
      "🔗 Created version '3' of model 'rag.development.langgraph_mcp_agent': https://adb-3253299566947192.12.azuredatabricks.net/explore/data/models/rag/development/langgraph_mcp_agent/version/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model registered: rag.development.langgraph_mcp_agent (version 3)\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "import os\n",
    "\n",
    "# Set registry URI with profile (same format as tracking URI)\n",
    "profile_name = os.getenv(\"DATABRICKS_CONFIG_PROFILE\", \"development\")\n",
    "mlflow.set_registry_uri(f\"databricks-uc://{profile_name}\")\n",
    "print(f\"✓ Registry URI set to: databricks-uc://{profile_name}\")\n",
    "\n",
    "# TODO: define the catalog, schema, and model name for your UC model\n",
    "catalog = \"rag\"\n",
    "schema = \"development\"\n",
    "model_name = \"langgraph_mcp_agent\"\n",
    "UC_MODEL_NAME = f\"{catalog}.{schema}.{model_name}\"\n",
    "\n",
    "# Register the model to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, \n",
    "    name=UC_MODEL_NAME\n",
    ")\n",
    "\n",
    "print(f\"Model registered: {UC_MODEL_NAME} (version {uc_registered_model_info.version})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a18fed",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "## Step 7: Deploy the Agent\n",
    "\n",
    "Deploy the agent to a model serving endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0685284b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00,  2.55it/s]\n",
      "\n",
      "2025/11/08 22:45:24 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/11/08 22:45:24 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "🔗 View Logged Model at: https://adb-3253299566947192.12.azuredatabricks.net/ml/experiments/897801252548174/models/m-530aa6d09e834f0a8f62df2744791055\n",
      "🔗 View Logged Model at: https://adb-3253299566947192.12.azuredatabricks.net/ml/experiments/897801252548174/models/m-530aa6d09e834f0a8f62df2744791055\n",
      "2025/11/08 22:45:25 INFO mlflow.pyfunc: Validating input example against model signature\n",
      "2025/11/08 22:45:25 INFO mlflow.pyfunc: Validating input example against model signature\n",
      "Successfully registered model 'rag.development.feedback'.\n",
      "Successfully registered model 'rag.development.feedback'.\n",
      "Uploading artifacts: 100%|██████████| 8/8 [00:00<00:00, 19.98it/s]\n",
      "\n",
      "🔗 Created version '1' of model 'rag.development.feedback': https://adb-3253299566947192.12.azuredatabricks.net/explore/data/models/rag/development/feedback/version/1\n",
      "🔗 Created version '1' of model 'rag.development.feedback': https://adb-3253299566947192.12.azuredatabricks.net/explore/data/models/rag/development/feedback/version/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run feedback-model at: https://adb-3253299566947192.12.azuredatabricks.net/ml/experiments/897801252548174/runs/41a7418d740a4bebaf00dfdeb3c1c693\n",
      "🧪 View experiment at: https://adb-3253299566947192.12.azuredatabricks.net/ml/experiments/897801252548174\n",
      "\n",
      "    Deployment of rag.development.langgraph_mcp_agent version 3 initiated.  This can take up to 15 minutes and the Review App & Query Endpoint will not work until this deployment finishes.\n",
      "\n",
      "    View status: https://adb-3253299566947192.12.azuredatabricks.net/ml/endpoints/agents_rag-development-langgraph_mcp_agent/\n",
      "    Review App: https://adb-3253299566947192.12.azuredatabricks.net/ml/review-v2/34911c7cde294e0bbfafd6c85f583473/chat\n",
      "    Monitor: https://adb-3253299566947192.12.azuredatabricks.net/ml/experiments/897801252548174?compareRunsMode=TRACES\n",
      "\n",
      "You can refer back to the links above from the endpoint detail page at https://adb-3253299566947192.12.azuredatabricks.net/ml/endpoints/agents_rag-development-langgraph_mcp_agent/.\n",
      "Agent deployed successfully!\n",
      "Endpoint: Deployment(model_name='rag.development.langgraph_mcp_agent', model_version='3', endpoint_name='agents_rag-development-langgraph_mcp_agent', served_entity_name='rag-development-langgraph_mcp_agent_3', query_endpoint='https://adb-3253299566947192.12.azuredatabricks.net/serving-endpoints/agents_rag-development-langgraph_mcp_agent/served-models/rag-development-langgraph_mcp_agent_3/invocations', endpoint_url='https://adb-3253299566947192.12.azuredatabricks.net/ml/endpoints/agents_rag-development-langgraph_mcp_agent/', review_app_url='https://adb-3253299566947192.12.azuredatabricks.net/ml/review-v2/34911c7cde294e0bbfafd6c85f583473/chat')\n",
      "\n",
      "    Deployment of rag.development.langgraph_mcp_agent version 3 initiated.  This can take up to 15 minutes and the Review App & Query Endpoint will not work until this deployment finishes.\n",
      "\n",
      "    View status: https://adb-3253299566947192.12.azuredatabricks.net/ml/endpoints/agents_rag-development-langgraph_mcp_agent/\n",
      "    Review App: https://adb-3253299566947192.12.azuredatabricks.net/ml/review-v2/34911c7cde294e0bbfafd6c85f583473/chat\n",
      "    Monitor: https://adb-3253299566947192.12.azuredatabricks.net/ml/experiments/897801252548174?compareRunsMode=TRACES\n",
      "\n",
      "You can refer back to the links above from the endpoint detail page at https://adb-3253299566947192.12.azuredatabricks.net/ml/endpoints/agents_rag-development-langgraph_mcp_agent/.\n",
      "Agent deployed successfully!\n",
      "Endpoint: Deployment(model_name='rag.development.langgraph_mcp_agent', model_version='3', endpoint_name='agents_rag-development-langgraph_mcp_agent', served_entity_name='rag-development-langgraph_mcp_agent_3', query_endpoint='https://adb-3253299566947192.12.azuredatabricks.net/serving-endpoints/agents_rag-development-langgraph_mcp_agent/served-models/rag-development-langgraph_mcp_agent_3/invocations', endpoint_url='https://adb-3253299566947192.12.azuredatabricks.net/ml/endpoints/agents_rag-development-langgraph_mcp_agent/', review_app_url='https://adb-3253299566947192.12.azuredatabricks.net/ml/review-v2/34911c7cde294e0bbfafd6c85f583473/chat')\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from databricks import agents\n",
    "\n",
    "deployment_info = agents.deploy(\n",
    "    UC_MODEL_NAME, \n",
    "    uc_registered_model_info.version,\n",
    "    scale_to_zero_enabled=True,\n",
    "    tags={\"endpointSource\": \"langgraph_mcp_notebook\"}\n",
    ")\n",
    "\n",
    "print(f\"Agent deployed successfully!\")\n",
    "print(f\"Endpoint: {deployment_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ecfa8b",
   "metadata": {},
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After your agent is deployed, you can:\n",
    "- Chat with it in AI Playground\n",
    "- Share it with SMEs for feedback\n",
    "- Embed it in a production application\n",
    "- Monitor its performance and traces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
