# Default configuration for LangGraph MCP Agent
# These are the baseline defaults - can be overridden by environment-specific configs

# Model/LLM Configuration
model:
  endpoint_name: "databricks-claude-3-7-sonnet"
  system_prompt: "You are a helpful AI assistant with access to various tools."
  temperature: 0.7
  max_tokens: 4096
  
  # MLflow Prompt Registry Configuration
  # Set use_prompt_registry to true to load system prompt from MLflow Prompt Registry
  use_prompt_registry: true
  prompt_name: "agent-system-prompt"
  prompt_version: "latest"  # e.g., 1, or use "latest" for @latest alias

# MCP Server Configuration
mcp:
  managed_server_urls: []  # Will be set dynamically based on workspace
  custom_server_urls: []
  timeout: 30

# MLflow Configuration
mlflow:
  # Use MLFLOW_EXPERIMENT_NAME env var set by Databricks jobs
  # Falls back to /Shared/langgraph-mcp-agent for local development
  experiment_name: "/Shared/langgraph-mcp-agent"  # Default for local dev
  model_name: "langgraph_mcp_agent"
  run_name: "agent_run"
  
  # ML Application metadata (consolidated from mlflow.yaml)
  ml_application:
    name: "langgraph-mcp-agent"
    description: "LangGraph agent with MCP tool calling capabilities"
    entry:
      file: "app.py"
      function: "create_agent"
    requirements:
      - "databricks-mcp"
      - "langgraph"
      - "mcp"
      - "databricks-langchain"
      - "databricks-sdk"
      - "nest-asyncio"
      - "pydantic"

# Unity Catalog Configuration
unity_catalog:
  catalog_name: "main"
  schema_name: "default"

# Databricks Configuration
databricks:
  workspace_url: null  # Auto-detected from environment
  auth_type: "default"  # default, token, oauth

# Deployment Configuration
deployment:
  environment: "dev"
  log_level: "INFO"
  enable_tracing: true
  enable_mlflow_autolog: true

# Agent Behavior
agent:
  max_iterations: 10
  recursion_limit: 25
  enable_tool_validation: true
  stream_response: true
