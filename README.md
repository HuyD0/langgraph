# LangGraph MCP Agent# LangGraph MCP Agent# langgraph



A production-ready LangGraph agent with Model Context Protocol (MCP) integration for Databricks.



## ğŸš€ Quick StartA production-ready LangGraph agent with Model Context Protocol (MCP) integration for Databricks.The 'langgraph' project was generated by using the default template.



```bash

# 1. Install dependencies

uv sync --dev## ğŸš€ Quick Start* `src/`: Python source code for this project.



# 2. Validate bundle  * `src/langgraph/`: Shared Python code that can be used by jobs and pipelines.

databricks bundle validate

1. **Read the docs**: Start with [`docs/START_HERE.md`](docs/START_HERE.md)* `resources/`:  Resource configurations (jobs, pipelines, etc.)

# 3. Deploy to dev

databricks bundle deploy -t dev2. **Setup**: Follow [`docs/GETTING_STARTED.md`](docs/GETTING_STARTED.md)* `tests/`: Unit tests for the shared Python code.



# 4. Run a job3. **Deploy**: Use [`docs/QUICKSTART_DAB.md`](docs/QUICKSTART_DAB.md)* `fixtures/`: Fixtures for data sets (primarily used for testing).

databricks bundle run agent_evaluation -t dev

```



ğŸ“š **Documentation**: See [`docs/START_HERE.md`](docs/START_HERE.md) for detailed setup## ğŸ“‚ Project Structure



## ğŸ“‚ Project Structure## Getting started



``````

lg-demo/

â”œâ”€â”€ src/langgraph_agent/          # Main packagelg-demo/Choose how you want to work on this project:

â”‚   â”œâ”€â”€ models/                   # Pydantic models & configs

â”‚   â”œâ”€â”€ core/                     # Agent logicâ”œâ”€â”€ src/langgraph_agent/          # Main package

â”‚   â”œâ”€â”€ utils/                    # Utilities

â”‚   â”œâ”€â”€ cli.py                    # CLI toolâ”‚   â”œâ”€â”€ models/                   # Pydantic models & configs â­ NEW(a) Directly in your Databricks workspace, see

â”‚   â”œâ”€â”€ evaluate.py               # Evaluation

â”‚   â””â”€â”€ deploy.py                 # Deploymentâ”‚   â”œâ”€â”€ core/                     # Agent logic    https://docs.databricks.com/dev-tools/bundles/workspace.

â”œâ”€â”€ resources/                     # DAB resources

â”œâ”€â”€ databricks.yml                # Bundle configurationâ”‚   â”œâ”€â”€ utils/                    # Utilities

â”œâ”€â”€ tests/                        # Test suite

â””â”€â”€ docs/                         # Documentationâ”‚   â”œâ”€â”€ cli.py                    # CLI tool(b) Locally with an IDE like Cursor or VS Code, see

```

â”‚   â”œâ”€â”€ evaluate.py               # Evaluation    https://docs.databricks.com/dev-tools/vscode-ext.html.

## âœ¨ Key Features

â”‚   â””â”€â”€ deploy.py                 # Deployment

- ğŸš€ **Serverless Single-Node Compute** - 70% faster startup, 50% cost savings

- ğŸ—ï¸ **Infrastructure as Code** - Databricks Asset Bundlesâ”œâ”€â”€ resources/                     # DAB resources(c) With command line tools, see https://docs.databricks.com/dev-tools/cli/databricks-cli.html

- ğŸ¤– **LangGraph Agent** - MCP tool integration  

- ğŸ“Š **MLOps Ready** - MLflow + Unity Catalogâ”œâ”€â”€ databricks.yml                # Bundle config

- ğŸ§ª **Modular Design** - Organized Pydantic models

â”œâ”€â”€ tests/                        # TestsIf you're developing with an IDE, dependencies for this project should be installed using uv:

## ğŸ› ï¸ Development

â”œâ”€â”€ docs/                         # Documentation â­ NEW

### Setup

â””â”€â”€ README.md                     # This file*  Make sure you have the UV package manager installed.

```bash

# Using uv (recommended)```   It's an alternative to tools like pip: https://docs.astral.sh/uv/getting-started/installation/.

uv sync --dev

*  Run `uv sync --dev` to install the project's dependencies.

# Or using pip

pip install -e ".[dev]"## âœ¨ Key Features

```



### Testing

- ğŸš€ **Serverless Compute** - 70% faster, 50% cheaper# Using this project using the CLI

```bash

make test          # Run all tests- ğŸ—ï¸ **Infrastructure as Code** - Databricks Asset Bundles

make test-unit     # Unit tests only

make test-cov      # With coverage- ğŸ¤– **LangGraph Agent** - MCP tool integrationThe Databricks workspace and IDE extensions provide a graphical interface for working

```

- ğŸ“Š **MLOps Ready** - MLflow + Unity Catalogwith this project. It's also possible to interact with it directly using the CLI:

### Code Quality



```bash

make lint          # Check code style## ğŸ› ï¸ Quick Commands1. Authenticate to your Databricks workspace, if you have not done so already:

make format        # Auto-format code

make clean         # Clean build artifacts    ```

```

```bash    $ databricks configure

## ğŸš¢ Deployment

# Validate configuration    ```

### Using Databricks Asset Bundles

databricks bundle validate

```bash

# Validate configuration2. To deploy a development copy of this project, type:

databricks bundle validate

# Deploy to dev (serverless)    ```

# Deploy to development

databricks bundle deploy -t devmake bundle-deploy-dev    $ databricks bundle deploy --target dev



# Run evaluation job    ```

databricks bundle run agent_evaluation -t dev

# Run evaluation    (Note that "dev" is the default target, so the `--target` parameter

# Run deployment job

databricks bundle run agent_deployment -t devmake bundle-run-eval    is optional here.)



# Deploy to production

databricks bundle deploy -t prod

# See all commands    This deploys everything that's defined for this project.

# Destroy environment

databricks bundle destroy -t devmake help    For example, the default template would deploy a pipeline called

```

```    `[dev yourname] langgraph_etl` to your workspace.

### Local Testing (Optional)

    You can find that resource by opening your workpace and clicking on **Jobs & Pipelines**.

```bash

# Quick agent test## ğŸ“š Documentation

python scripts/test_agent.py

3. Similarly, to deploy a production copy, type:

# Start local server

langgraph-agent serveAll documentation is in the [`docs/`](docs/) directory:   ```

```

   $ databricks bundle deploy --target prod

## ğŸ“š Documentation

- **Quick Start**: [`docs/START_HERE.md`](docs/START_HERE.md)   ```

All documentation is in [`docs/`](docs/):

- **Setup Guide**: [`docs/GETTING_STARTED.md`](docs/GETTING_STARTED.md)   Note the default template has a includes a job that runs the pipeline every day

- **[START_HERE.md](docs/START_HERE.md)** - Quick overview

- **[GETTING_STARTED.md](docs/GETTING_STARTED.md)** - Setup guide- **DAB Guide**: [`docs/DATABRICKS_BUNDLES.md`](docs/DATABRICKS_BUNDLES.md)   (defined in resources/sample_job.job.yml). The schedule

- **[DATABRICKS_BUNDLES.md](docs/DATABRICKS_BUNDLES.md)** - Complete DAB guide

- **[SERVERLESS_UPDATE_SUMMARY.md](docs/SERVERLESS_UPDATE_SUMMARY.md)** - Serverless features- **Serverless**: [`docs/SERVERLESS_UPDATE_SUMMARY.md`](docs/SERVERLESS_UPDATE_SUMMARY.md)   is paused when deploying in development mode (see

- **[PROJECT_README.md](docs/PROJECT_README.md)** - Full API reference

- **Full Reference**: [`docs/PROJECT_README.md`](docs/PROJECT_README.md)   https://docs.databricks.com/dev-tools/bundles/deployment-modes.html).

See [`docs/README.md`](docs/README.md) for complete index.



## ğŸ—ï¸ Architecture

See [`docs/README.md`](docs/README.md) for complete documentation index.4. To run a job or pipeline, use the "run" command:

### Pydantic Models

   ```

Configuration organized in `src/langgraph_agent/models/`:

## ğŸ—ï¸ Architecture Highlights   $ databricks bundle run

```python

from langgraph_agent.models import (   ```

    AgentConfig,        # Main configuration

    AgentState,         # Agent state### Modular Pydantic Models

    ModelConfig,        # LLM settings

    DatabricksConfig,   # Databricks settings5. Finally, to run tests locally, use `pytest`:

    UnityCatalogConfig, # UC settings

    get_config,         # Config loaderConfiguration is organized in `src/langgraph_agent/models/`:   ```

)

```   $ uv run pytest



### Serverless Compute```python   ```



- âš¡ 1-2 minute job startup (vs 5-8 min classic)from langgraph_agent.models import (

- ğŸ’° ~50% cost reduction    AgentConfig,        # Main configuration

- ğŸ”§ Auto-configured single-node clusters    AgentState,         # Agent state

- ğŸ“¦ Auto-selected ML runtime    ModelConfig,        # LLM settings

    DatabricksConfig,   # Databricks settings

### DAB Structure    UnityCatalogConfig, # UC settings

    get_config,         # Config loader

```yaml)

databricks.yml              # Main bundle config```

resources/

  â”œâ”€â”€ agent_jobs.yml        # Job definitions### Serverless Single-Node

  â”œâ”€â”€ agent_serving.py      # Serving endpoints

  â”œâ”€â”€ experiments.py        # MLflow experimentsJobs use optimized single-node clusters:

  â””â”€â”€ jobs.py               # Python job definitions- âš¡ 1-2 minute startup

```- ğŸ’° 50% cost reduction

- ğŸ”§ Auto-configured runtime

## ğŸ”§ Configuration

### Databricks Asset Bundles

Via environment variables or `.env` file:

Deploy everything with one command:

```bash```bash

# Databricksdatabricks bundle deploy -t dev

DATABRICKS_PROFILE=development```

DATABRICKS_HOST=https://your-workspace.azuredatabricks.net

## ğŸ§ª Testing

# Model

MODEL_ENDPOINT_NAME=databricks-claude-3-7-sonnet```bash

# Run all tests

# Unity Catalogmake test

UC_CATALOG=rag

UC_SCHEMA=development# Run with coverage

UC_MODEL_NAME=langgraph_mcp_agentmake test-cov

```

# Unit tests only

See [`configs/.env.example`](configs/.env.example) for template.make test-unit

```

## ğŸ§ª Testing

## ğŸš¢ Deployment

```bash

# All tests### Databricks Asset Bundles (Recommended)

make test

DAB handles everything - no scripts needed!

# Unit tests only

make test-unit```bash

# Validate configuration

# With coverage reportdatabricks bundle validate

make test-cov

```# Deploy to dev

databricks bundle deploy -t dev

Tests are in `tests/unit/` and `tests/integration/`.

# Run jobs

## ğŸ¤ Contributingdatabricks bundle run agent_deployment -t dev

databricks bundle run agent_evaluation -t dev

1. Create feature branch

2. Make changes# Deploy to prod

3. Run tests: `make test`databricks bundle deploy -t prod

4. Check code style: `make lint````

5. Validate bundle: `databricks bundle validate`

6. Submit PR### Local Testing (Optional)



## ğŸ“ Recent UpdatesFor quick local testing before deploying:



- **Nov 8, 2025**: Simplified tooling - DAB replaces shell scripts```bash

- **Nov 8, 2025**: Reorganized with dedicated `models/` module# Test agent locally

- **Nov 8, 2025**: Migrated to serverless single-node computepython scripts/test_agent.py

- Added Databricks Asset Bundles support

- Complete project transformation from notebook# Or use the CLI

langgraph-agent serve

## ğŸ†˜ Support```



- **Documentation**: [`docs/`](docs/)## ğŸ“¦ Installation

- **Quick Help**: `make help`

- **DAB Help**: `databricks bundle --help````bash

- **Issues**: GitHub Issues# Clone and setup

git clone <repo>

---cd lg-demo



**Ready to deploy?** Run `databricks bundle deploy -t dev` ğŸš€# Install dependencies with uv

uv sync --dev

# Or with pip
pip install -e ".[dev]"
```

## ğŸ”§ Configuration

Configuration via environment variables or `.env` file:

```bash
# Databricks
DATABRICKS_PROFILE=development
DATABRICKS_HOST=https://your-workspace.azuredatabricks.net

# Model
MODEL_ENDPOINT_NAME=databricks-claude-3-7-sonnet

# Unity Catalog
UC_CATALOG=rag
UC_SCHEMA=development
UC_MODEL_NAME=langgraph_mcp_agent
```

See [`configs/.env.example`](configs/.env.example) for full template.

## ğŸ“ Recent Updates

- **Nov 8, 2025**: Reorganized with dedicated `models/` module
- **Nov 8, 2025**: Migrated to serverless single-node compute
- Added Databricks Asset Bundles support
- Complete project transformation from notebook

## ğŸ¤ Contributing

1. Create feature branch
2. Make changes
3. Run tests: `make test`
4. Validate bundle: `make bundle-validate`
5. Submit PR

## ğŸ“„ License

See LICENSE file.

## ğŸ†˜ Support

- Documentation: [`docs/`](docs/)
- Issues: GitHub Issues
- Quick help: `make help`

---

**Ready to deploy?** Run `make bundle-deploy-dev` ğŸš€
